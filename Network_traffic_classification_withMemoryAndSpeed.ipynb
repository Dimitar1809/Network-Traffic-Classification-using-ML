{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [15 lines of output]\n",
      "      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "      rather than 'sklearn' for pip commands.\n",
      "      \n",
      "      Here is how to fix this error in the main use cases:\n",
      "      - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "      - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "      - if the 'sklearn' package is used by one of your dependencies,\n",
      "        it would be great if you take some time to track which package uses\n",
      "        'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "      - as a last resort, set the environment variable\n",
      "        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "      \n",
      "      More information is available at\n",
      "      https://github.com/scikit-learn/sklearn-pypi-package\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: memory_profiler in c:\\users\\mitak\\anaconda3\\lib\\site-packages (0.61.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\mitak\\anaconda3\\lib\\site-packages (from memory_profiler) (5.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting tensorflow\n",
      "  Obtaining dependency information for tensorflow from https://files.pythonhosted.org/packages/e4/14/d795bb156f8cc10eb1dcfe1332b7dbb8405b634688980aa9be8f885cc888/tensorflow-2.16.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading tensorflow-2.16.1-cp311-cp311-win_amd64.whl.metadata (3.5 kB)\n",
      "Collecting tensorflow-intel==2.16.1 (from tensorflow)\n",
      "  Obtaining dependency information for tensorflow-intel==2.16.1 from https://files.pythonhosted.org/packages/e0/36/6278e4e7e69a90c00e0f82944d8f2713dd85a69d1add455d9e50446837ab/tensorflow_intel-2.16.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading tensorflow_intel-2.16.1-cp311-cp311-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\mitak\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for astunparse>=1.6.0 from https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl.metadata\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for flatbuffers>=23.5.26 from https://files.pythonhosted.org/packages/41/f0/7e988a019bc54b2dbd0ad4182ef2d53488bb02e58694cd79d61369e85900/flatbuffers-24.3.25-py2.py3-none-any.whl.metadata\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 from https://files.pythonhosted.org/packages/fa/39/5aae571e5a5f4de9c3445dae08a530498e5c53b0e74410eeeb0991c79047/gast-0.5.4-py3-none-any.whl.metadata\n",
      "  Downloading gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for google-pasta>=0.1.1 from https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl.metadata\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for h5py>=3.10.0 from https://files.pythonhosted.org/packages/d8/5e/b7b83cfe60504cc4d24746aed04353af7ea8ec104e597e5ae71b8d0390cb/h5py-3.11.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading h5py-3.11.0-cp311-cp311-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for libclang>=13.0.0 from https://files.pythonhosted.org/packages/0b/2d/3f480b1e1d31eb3d6de5e3ef641954e5c67430d5ac93b7fa7e07589576c7/libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for ml-dtypes~=0.3.1 from https://files.pythonhosted.org/packages/a4/db/1784b87285588788170f87e987bfb4bda218d62a70a81ebb66c94e7f9b95/ml_dtypes-0.3.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading ml_dtypes-0.3.2-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for opt-einsum>=2.3.2 from https://files.pythonhosted.org/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl.metadata\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\mitak\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (23.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 from https://files.pythonhosted.org/packages/ad/6e/1bed3b7c904cc178cb8ee8dbaf72934964452b3de95b7a63412591edb93c/protobuf-4.25.3-cp310-abi3-win_amd64.whl.metadata\n",
      "  Downloading protobuf-4.25.3-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\mitak\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mitak\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\mitak\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for termcolor>=1.1.0 from https://files.pythonhosted.org/packages/d9/5f/8c716e47b3a50cbd7c146f45881e11d9414def768b7cd9c5e6650ec2a80a/termcolor-2.4.0-py3-none-any.whl.metadata\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\mitak\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\mitak\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.14.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for grpcio<2.0,>=1.24.3 from https://files.pythonhosted.org/packages/bc/4f/22cdc1d2073593aac4c0cfee7d69418c6dad24069372e3f6e6706673daa6/grpcio-1.64.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading grpcio-1.64.0-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for tensorboard<2.17,>=2.16 from https://files.pythonhosted.org/packages/3a/d0/b97889ffa769e2d1fdebb632084d5e8b53fc299d43a537acee7ec0c021a3/tensorboard-2.16.2-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\mitak\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.3)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for tensorflow-io-gcs-filesystem>=0.23.1 from https://files.pythonhosted.org/packages/ac/4e/9566a313927be582ca99455a9523a097c7888fc819695bdc08415432b202/tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\mitak\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\mitak\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: rich in c:\\users\\mitak\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\mitak\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\mitak\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mitak\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mitak\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mitak\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mitak\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\mitak\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.4.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/7a/13/e503968fefabd4c6b2650af21e110aa8466fe21432cd7c43a84577a89438/tensorboard_data_server-0.7.2-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\mitak\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\mitak\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\mitak\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\mitak\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\mitak\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.0)\n",
      "Downloading tensorflow-2.16.1-cp311-cp311-win_amd64.whl (2.1 kB)\n",
      "Downloading tensorflow_intel-2.16.1-cp311-cp311-win_amd64.whl (377.0 MB)\n",
      "   ---------------------------------------- 0.0/377.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/377.0 MB 8.3 MB/s eta 0:00:46\n",
      "   ---------------------------------------- 0.7/377.0 MB 9.3 MB/s eta 0:00:41\n",
      "   ---------------------------------------- 1.4/377.0 MB 12.2 MB/s eta 0:00:31\n",
      "   ---------------------------------------- 2.0/377.0 MB 12.5 MB/s eta 0:00:31\n",
      "   ---------------------------------------- 2.8/377.0 MB 13.8 MB/s eta 0:00:28\n",
      "   ---------------------------------------- 3.7/377.0 MB 15.7 MB/s eta 0:00:24\n",
      "    --------------------------------------- 5.1/377.0 MB 18.1 MB/s eta 0:00:21\n",
      "    --------------------------------------- 6.4/377.0 MB 20.4 MB/s eta 0:00:19\n",
      "    --------------------------------------- 7.8/377.0 MB 20.9 MB/s eta 0:00:18\n",
      "   - -------------------------------------- 9.7/377.0 MB 23.0 MB/s eta 0:00:16\n",
      "   - -------------------------------------- 11.8/377.0 MB 29.7 MB/s eta 0:00:13\n",
      "   - -------------------------------------- 13.9/377.0 MB 36.4 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 16.0/377.0 MB 40.9 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 17.8/377.0 MB 43.7 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 19.9/377.0 MB 40.9 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 21.3/377.0 MB 43.5 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 23.3/377.0 MB 43.5 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 24.4/377.0 MB 38.6 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 26.5/377.0 MB 40.9 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 28.2/377.0 MB 40.9 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 30.3/377.0 MB 40.9 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 32.1/377.0 MB 38.6 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 33.9/377.0 MB 40.9 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 35.9/377.0 MB 46.9 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 37.5/377.0 MB 43.7 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 39.7/377.0 MB 43.7 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 41.4/377.0 MB 40.9 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 43.2/377.0 MB 43.7 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 45.0/377.0 MB 40.9 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 47.0/377.0 MB 43.5 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 49.2/377.0 MB 43.5 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 50.4/377.0 MB 40.9 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 52.4/377.0 MB 40.9 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 54.0/377.0 MB 38.5 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 55.8/377.0 MB 38.6 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 57.6/377.0 MB 40.9 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 59.9/377.0 MB 43.5 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 62.2/377.0 MB 46.9 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 63.4/377.0 MB 46.7 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 63.6/377.0 MB 34.4 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 67.2/377.0 MB 40.9 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 68.1/377.0 MB 40.9 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 69.3/377.0 MB 34.4 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 71.3/377.0 MB 34.4 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 73.6/377.0 MB 34.4 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 75.2/377.0 MB 38.6 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 77.0/377.0 MB 38.5 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 78.5/377.0 MB 40.9 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 80.6/377.0 MB 40.9 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 82.3/377.0 MB 40.9 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 83.8/377.0 MB 38.6 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 85.9/377.0 MB 40.9 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 87.5/377.0 MB 38.6 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 89.4/377.0 MB 40.9 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 91.8/377.0 MB 40.9 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 94.2/377.0 MB 46.7 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 96.0/377.0 MB 43.5 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 97.9/377.0 MB 46.7 MB/s eta 0:00:06\n",
      "   ---------- ---------------------------- 100.1/377.0 MB 50.4 MB/s eta 0:00:06\n",
      "   ---------- ---------------------------- 102.0/377.0 MB 46.7 MB/s eta 0:00:06\n",
      "   ---------- ---------------------------- 103.2/377.0 MB 43.5 MB/s eta 0:00:07\n",
      "   ---------- ---------------------------- 104.2/377.0 MB 36.3 MB/s eta 0:00:08\n",
      "   ---------- ---------------------------- 105.3/377.0 MB 34.6 MB/s eta 0:00:08\n",
      "   ---------- ---------------------------- 106.2/377.0 MB 32.8 MB/s eta 0:00:09\n",
      "   ----------- --------------------------- 107.3/377.0 MB 29.7 MB/s eta 0:00:10\n",
      "   ----------- --------------------------- 109.2/377.0 MB 29.7 MB/s eta 0:00:10\n",
      "   ----------- --------------------------- 110.9/377.0 MB 28.5 MB/s eta 0:00:10\n",
      "   ----------- --------------------------- 112.9/377.0 MB 29.7 MB/s eta 0:00:09\n",
      "   ----------- --------------------------- 114.4/377.0 MB 32.8 MB/s eta 0:00:09\n",
      "   ------------ -------------------------- 116.5/377.0 MB 38.5 MB/s eta 0:00:07\n",
      "   ------------ -------------------------- 117.8/377.0 MB 38.5 MB/s eta 0:00:07\n",
      "   ------------ -------------------------- 120.0/377.0 MB 40.9 MB/s eta 0:00:07\n",
      "   ------------ -------------------------- 122.2/377.0 MB 40.9 MB/s eta 0:00:07\n",
      "   ------------ -------------------------- 124.0/377.0 MB 43.5 MB/s eta 0:00:06\n",
      "   ------------- ------------------------- 126.0/377.0 MB 43.5 MB/s eta 0:00:06\n",
      "   ------------- ------------------------- 127.7/377.0 MB 43.5 MB/s eta 0:00:06\n",
      "   ------------- ------------------------- 130.0/377.0 MB 50.4 MB/s eta 0:00:05\n",
      "   ------------- ------------------------- 131.7/377.0 MB 46.7 MB/s eta 0:00:06\n",
      "   ------------- ------------------------- 133.0/377.0 MB 43.5 MB/s eta 0:00:06\n",
      "   ------------- ------------------------- 135.0/377.0 MB 43.7 MB/s eta 0:00:06\n",
      "   -------------- ------------------------ 136.8/377.0 MB 38.5 MB/s eta 0:00:07\n",
      "   -------------- ------------------------ 138.9/377.0 MB 40.9 MB/s eta 0:00:06\n",
      "   -------------- ------------------------ 140.5/377.0 MB 36.3 MB/s eta 0:00:07\n",
      "   -------------- ------------------------ 142.5/377.0 MB 38.5 MB/s eta 0:00:07\n",
      "   -------------- ------------------------ 144.8/377.0 MB 40.9 MB/s eta 0:00:06\n",
      "   --------------- ----------------------- 147.1/377.0 MB 43.7 MB/s eta 0:00:06\n",
      "   --------------- ----------------------- 148.6/377.0 MB 43.5 MB/s eta 0:00:06\n",
      "   --------------- ----------------------- 150.5/377.0 MB 43.7 MB/s eta 0:00:06\n",
      "   --------------- ----------------------- 152.5/377.0 MB 40.9 MB/s eta 0:00:06\n",
      "   ---------------- ---------------------- 154.8/377.0 MB 40.9 MB/s eta 0:00:06\n",
      "   ---------------- ---------------------- 156.5/377.0 MB 43.7 MB/s eta 0:00:06\n",
      "   ---------------- ---------------------- 158.8/377.0 MB 43.7 MB/s eta 0:00:05\n",
      "   ---------------- ---------------------- 159.8/377.0 MB 43.7 MB/s eta 0:00:05\n",
      "   ---------------- ---------------------- 161.6/377.0 MB 40.9 MB/s eta 0:00:06\n",
      "   ---------------- ---------------------- 163.8/377.0 MB 40.9 MB/s eta 0:00:06\n",
      "   ----------------- --------------------- 165.4/377.0 MB 38.6 MB/s eta 0:00:06\n",
      "   ----------------- --------------------- 167.6/377.0 MB 43.5 MB/s eta 0:00:05\n",
      "   ----------------- --------------------- 169.5/377.0 MB 38.6 MB/s eta 0:00:06\n",
      "   ----------------- --------------------- 171.3/377.0 MB 43.7 MB/s eta 0:00:05\n",
      "   ----------------- --------------------- 173.3/377.0 MB 40.9 MB/s eta 0:00:05\n",
      "   ------------------ -------------------- 175.0/377.0 MB 43.7 MB/s eta 0:00:05\n",
      "   ------------------ -------------------- 176.9/377.0 MB 40.9 MB/s eta 0:00:05\n",
      "   ------------------ -------------------- 178.7/377.0 MB 40.9 MB/s eta 0:00:05\n",
      "   ------------------ -------------------- 180.7/377.0 MB 40.9 MB/s eta 0:00:05\n",
      "   ------------------ -------------------- 182.1/377.0 MB 40.9 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 184.0/377.0 MB 38.5 MB/s eta 0:00:06\n",
      "   ------------------- ------------------- 185.9/377.0 MB 40.9 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 187.7/377.0 MB 38.5 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 189.9/377.0 MB 40.9 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 191.4/377.0 MB 40.9 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 193.3/377.0 MB 43.5 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 195.1/377.0 MB 43.5 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 197.3/377.0 MB 43.7 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 199.3/377.0 MB 40.9 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 201.1/377.0 MB 43.7 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 202.8/377.0 MB 40.9 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 204.5/377.0 MB 38.5 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 206.5/377.0 MB 43.7 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 207.6/377.0 MB 43.7 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 209.9/377.0 MB 40.9 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 211.4/377.0 MB 38.5 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 212.8/377.0 MB 38.5 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 214.0/377.0 MB 34.4 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 215.8/377.0 MB 34.6 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 217.3/377.0 MB 36.3 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 219.1/377.0 MB 34.4 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 219.9/377.0 MB 31.2 MB/s eta 0:00:06\n",
      "   ---------------------- ---------------- 221.5/377.0 MB 31.2 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 223.1/377.0 MB 32.8 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 225.0/377.0 MB 36.4 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 226.8/377.0 MB 34.4 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 228.9/377.0 MB 36.4 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 231.1/377.0 MB 43.5 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 232.3/377.0 MB 40.9 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 234.4/377.0 MB 40.9 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 236.1/377.0 MB 40.9 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 237.8/377.0 MB 40.9 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 240.2/377.0 MB 40.9 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 242.2/377.0 MB 40.9 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 243.8/377.0 MB 40.9 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 245.7/377.0 MB 40.9 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 247.5/377.0 MB 43.7 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 249.4/377.0 MB 43.7 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 250.3/377.0 MB 38.5 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 252.2/377.0 MB 40.9 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 254.2/377.0 MB 40.9 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 255.9/377.0 MB 38.5 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 257.8/377.0 MB 40.9 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 259.5/377.0 MB 40.9 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 261.5/377.0 MB 43.7 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 262.4/377.0 MB 43.5 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 264.7/377.0 MB 43.7 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 266.4/377.0 MB 43.7 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 267.8/377.0 MB 40.9 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 269.5/377.0 MB 38.6 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 271.3/377.0 MB 38.5 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 272.8/377.0 MB 43.7 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 274.1/377.0 MB 38.6 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 274.7/377.0 MB 38.5 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 274.7/377.0 MB 38.5 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 275.0/377.0 MB 28.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 276.3/377.0 MB 27.3 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 277.6/377.0 MB 27.3 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 278.8/377.0 MB 27.3 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 280.3/377.0 MB 26.2 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 282.0/377.0 MB 27.3 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 283.6/377.0 MB 26.2 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 285.1/377.0 MB 36.4 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 286.8/377.0 MB 38.5 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 288.6/377.0 MB 38.5 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 290.1/377.0 MB 40.9 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 291.8/377.0 MB 40.9 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 293.4/377.0 MB 38.5 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 294.6/377.0 MB 38.5 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 296.4/377.0 MB 38.5 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 298.1/377.0 MB 38.5 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 299.8/377.0 MB 40.9 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 301.5/377.0 MB 38.5 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 302.9/377.0 MB 38.5 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 304.3/377.0 MB 38.5 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 305.8/377.0 MB 38.5 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 307.3/377.0 MB 36.3 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 309.1/377.0 MB 38.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 310.4/377.0 MB 36.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 312.1/377.0 MB 38.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 313.8/377.0 MB 38.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 315.5/377.0 MB 38.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 317.1/377.0 MB 38.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 318.8/377.0 MB 40.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 320.4/377.0 MB 38.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 321.5/377.0 MB 38.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 323.1/377.0 MB 36.4 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 324.9/377.0 MB 38.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 326.6/377.0 MB 36.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 328.3/377.0 MB 38.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 329.8/377.0 MB 36.3 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 331.5/377.0 MB 38.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 333.5/377.0 MB 40.9 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 334.8/377.0 MB 38.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 336.6/377.0 MB 40.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 338.4/377.0 MB 40.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 340.2/377.0 MB 40.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 342.0/377.0 MB 40.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 343.4/377.0 MB 40.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 345.2/377.0 MB 40.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 346.5/377.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 347.9/377.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 349.5/377.0 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 351.2/377.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 353.0/377.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 354.7/377.0 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 356.2/377.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 357.9/377.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 359.5/377.0 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 361.3/377.0 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 362.6/377.0 MB 36.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 364.4/377.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 366.0/377.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 366.9/377.0 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  368.6/377.0 MB 38.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  370.3/377.0 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  372.2/377.0 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  373.5/377.0 MB 38.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  374.3/377.0 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  375.4/377.0 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  376.5/377.0 MB 31.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 32.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 32.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 32.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 32.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 32.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 32.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 32.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 32.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 32.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 32.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 32.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 32.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 32.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 32.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 32.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 32.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 32.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 377.0/377.0 MB 7.8 MB/s eta 0:00:00\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "   ---------------------------------------- 0.0/57.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 57.5/57.5 kB 3.0 MB/s eta 0:00:00\n",
      "Downloading grpcio-1.64.0-cp311-cp311-win_amd64.whl (4.1 MB)\n",
      "   ---------------------------------------- 0.0/4.1 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 1.3/4.1 MB 40.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 3.0/4.1 MB 47.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  4.1/4.1 MB 43.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.1/4.1 MB 37.1 MB/s eta 0:00:00\n",
      "Downloading h5py-3.11.0-cp311-cp311-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ------------------------- -------------- 1.9/3.0 MB 40.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.0/3.0 MB 38.1 MB/s eta 0:00:00\n",
      "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "   ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.7/26.4 MB 55.2 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 3.6/26.4 MB 45.9 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 5.0/26.4 MB 45.4 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 6.9/26.4 MB 43.8 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 8.4/26.4 MB 44.7 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 10.2/26.4 MB 43.5 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 11.9/26.4 MB 40.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 13.3/26.4 MB 40.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 15.1/26.4 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 16.5/26.4 MB 40.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 18.4/26.4 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 20.0/26.4 MB 38.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 21.7/26.4 MB 40.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 23.4/26.4 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 25.0/26.4 MB 38.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.2/26.4 MB 40.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 38.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.4/26.4 MB 31.2 MB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.3.2-cp311-cp311-win_amd64.whl (127 kB)\n",
      "   ---------------------------------------- 0.0/127.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 127.7/127.7 kB 7.3 MB/s eta 0:00:00\n",
      "Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "   ---------------------------------------- 0.0/65.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 65.5/65.5 kB ? eta 0:00:00\n",
      "Downloading protobuf-4.25.3-cp310-abi3-win_amd64.whl (413 kB)\n",
      "   ---------------------------------------- 0.0/413.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 413.4/413.4 kB 26.9 MB/s eta 0:00:00\n",
      "Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 1.1/5.5 MB 23.7 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.3/5.5 MB 28.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.3/5.5 MB 30.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 4.3/5.5 MB 27.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.2/5.5 MB 25.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 23.4 MB/s eta 0:00:00\n",
      "Downloading tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------------------------ --------------- 0.9/1.5 MB 29.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 23.4 MB/s eta 0:00:00\n",
      "Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Installing collected packages: libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, opt-einsum, ml-dtypes, h5py, grpcio, google-pasta, gast, astunparse, tensorboard, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml-dtypes 0.4.0\n",
      "    Uninstalling ml-dtypes-0.4.0:\n",
      "      Successfully uninstalled ml-dtypes-0.4.0\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.9.0\n",
      "    Uninstalling h5py-3.9.0:\n",
      "      Successfully uninstalled h5py-3.9.0\n",
      "Successfully installed astunparse-1.6.3 flatbuffers-24.3.25 gast-0.5.4 google-pasta-0.2.0 grpcio-1.64.0 h5py-3.11.0 libclang-18.1.1 ml-dtypes-0.3.2 opt-einsum-3.3.0 protobuf-4.25.3 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 tensorflow-intel-2.16.1 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "\n",
    "def process_network_traffic(data,  resample_interval):\n",
    "    # Convert TIME column to datetime format assuming it's in Unix timestamp format\n",
    "    data['TIME'] = pd.to_datetime(data['TIME'], unit='s')\n",
    "\n",
    "    # Set TIME as the index\n",
    "    data.set_index('TIME', inplace=True)\n",
    "\n",
    "    # Resample data in 15-second intervals\n",
    "    resampled_data = data.resample(resample_interval)\n",
    "\n",
    "    # Function to calculate flow attributes for each interval\n",
    "    def calculate_flow_attributes(interval):\n",
    "        if interval.empty:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Reset index to make 'TIME' column available\n",
    "        interval = interval.reset_index()\n",
    "\n",
    "        # Extract flows based on source IP, destination IP, and eth.src\n",
    "        flows = interval.groupby(['IP.src', 'IP.dst', 'eth.src'])\n",
    "\n",
    "        # Calculate additional features for each flow\n",
    "        flow_features = flows.agg(\n",
    "            total_packets=('Size', 'count'),\n",
    "            total_bytes=('Size', 'sum'),\n",
    "            start_time=('TIME', 'min'),\n",
    "            end_time=('TIME', 'max'),\n",
    "            mean_packet_size=('Size', 'mean'),\n",
    "            std_packet_size=('Size', 'std'),\n",
    "            min_packet_size=('Size', 'min'),\n",
    "            max_packet_size=('Size', 'max')\n",
    "        ).reset_index()\n",
    "\n",
    "        # Calculate flow duration\n",
    "        flow_features['flow_duration'] = (flow_features['end_time'] - flow_features['start_time']).dt.total_seconds()\n",
    "\n",
    "        # Calculate packet rate and byte rate\n",
    "        flow_features['packet_rate'] = flow_features['total_packets'] / flow_features['flow_duration']\n",
    "        flow_features['byte_rate'] = flow_features['total_bytes'] / flow_features['flow_duration']\n",
    "\n",
    "        # Handle cases where flow_duration is zero to avoid division by zero errors\n",
    "        flow_features.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "        # Calculate inter-arrival times\n",
    "        interval['inter_arrival_time'] = interval.groupby(['IP.src', 'IP.dst', 'eth.src'])['TIME'].diff().dt.total_seconds()\n",
    "\n",
    "        # Calculate statistical features of inter-arrival times for each flow\n",
    "        inter_arrival_stats = interval.groupby(['IP.src', 'IP.dst', 'eth.src'])['inter_arrival_time'].agg(\n",
    "            mean_inter_arrival=('mean'),\n",
    "            std_inter_arrival=('std'),\n",
    "            min_inter_arrival=('min'),\n",
    "            max_inter_arrival=('max')\n",
    "        ).reset_index()\n",
    "\n",
    "        # Merge the inter-arrival stats with the flow features\n",
    "        flow_features = pd.merge(flow_features, inter_arrival_stats, on=['IP.src', 'IP.dst', 'eth.src'])\n",
    "\n",
    "        # Fill NaN values with 0 (for flows with single packet where diff results in NaN)\n",
    "        flow_features.fillna(0, inplace=True)\n",
    "\n",
    "        return flow_features\n",
    "\n",
    "    # Apply the function to each interval\n",
    "    flow_attributes_list = []\n",
    "    for _, interval in resampled_data:\n",
    "        flow_attributes = calculate_flow_attributes(interval)\n",
    "        flow_attributes_list.append(flow_attributes)\n",
    "\n",
    "    # Combine all intervals into a single DataFrame\n",
    "    combined_flow_attributes = pd.concat(flow_attributes_list, ignore_index=True)\n",
    "\n",
    "    # Optional: Define the MAC address to device type mapping\n",
    "    mac_to_device = {\n",
    "        'd0:52:a8:00:67:5e': ('Smart Things', 'hubs/controllers'),\n",
    "        '44:65:0d:56:cc:d3': ('Amazon Echo', 'hubs/controllers'),\n",
    "        '70:ee:50:18:34:43': ('Netatmo Welcome', 'cameras'),\n",
    "        'f4:f2:6d:93:51:f1': ('TP-Link Day Night Cloud camera', 'cameras'),\n",
    "        '00:16:6c:ab:6b:88': ('Samsung SmartCam', 'cameras'),\n",
    "        '30:8c:fb:2f:e4:b2': ('Dropcam', 'cameras'),\n",
    "        '00:62:6e:51:27:2e': ('Insteon Camera', 'cameras'),\n",
    "        'e8:ab:fa:19:de:4f': ('Insteon Camera', 'cameras'),\n",
    "        '00:24:e4:11:18:a8': ('Withings Smart Baby Monitor', 'cameras'),\n",
    "        'ec:1a:59:79:f4:89': ('Belkin Wemo switch', 'energy management'),\n",
    "        '50:c7:bf:00:56:39': ('TP-Link Smart plug', 'energy management'),\n",
    "        '74:c6:3b:29:d7:1d': ('iHome', 'appliances'),\n",
    "        'ec:1a:59:83:28:11': ('Belkin wemo motion sensor', 'health-monitor'),\n",
    "        '18:b4:30:25:be:e4': ('NEST Protect smoke alarm', 'health-monitor'),\n",
    "        '70:ee:50:03:b8:ac': ('Netatmo weather station', 'health-monitor'),\n",
    "        '00:24:e4:1b:6f:96': ('Withings Smart scale', 'health-monitor'),\n",
    "        '74:6a:89:00:2e:25': ('Blipcare Blood Pressure meter', 'health-monitor'),\n",
    "        '00:24:e4:20:28:c6': ('Withings Aura smart sleep sensor', 'health-monitor'),\n",
    "        'd0:73:d5:01:83:08': ('Light Bulbs LiFX Smart Bulb', 'appliances'),\n",
    "        '18:b7:9e:02:20:44': ('Triby Speaker', 'appliances'),\n",
    "        'e0:76:d0:33:bb:85': ('PIX-STAR Photo-frame', 'appliances'),\n",
    "        '70:5a:0f:e4:9b:c0': ('HP Printer', 'appliances'),\n",
    "        '08:21:ef:3b:fc:e3': ('Samsung Galaxy Tab', 'non-IoT'),\n",
    "        '30:8c:fb:b6:ea:45': ('Nest Dropcam', 'cameras'),\n",
    "        '40:f3:08:ff:1e:da': ('Android Phone', 'non-IoT'),\n",
    "        '74:2f:68:81:69:42': ('Laptop', 'non-IoT'),\n",
    "        'ac:bc:32:d4:6f:2f': ('MacBook', 'non-IoT'),\n",
    "        'b4:ce:f6:a7:a3:c2': ('Android Phone', 'non-IoT'),\n",
    "        'd0:a6:37:df:a1:e1': ('IPhone', 'non-IoT'),\n",
    "        'f4:5c:89:93:cc:85': ('MacBook/Iphone', 'non-IoT'),\n",
    "        '14:cc:20:51:33:ea': ('TPLink Router Bridge LAN (Gateway)', 'Gateaway')\n",
    "    }\n",
    "\n",
    "    # Map the eth.src values to device and categories\n",
    "    combined_flow_attributes['Device'] = combined_flow_attributes['eth.src'].map(lambda mac: mac_to_device.get(mac, ('Unknown', 'Unknown'))[0])\n",
    "    combined_flow_attributes['Device Category'] = combined_flow_attributes['eth.src'].map(lambda mac: mac_to_device.get(mac, ('Unknown', 'Unknown'))[1])\n",
    "\n",
    "    # Remove rows where 'Device Category' is 'Gateaway'\n",
    "    combined_flow_attributes = combined_flow_attributes[combined_flow_attributes['Device Category'] != 'Gateaway']\n",
    "\n",
    "    return combined_flow_attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import joblib\n",
    "import os\n",
    "from memory_profiler import memory_usage\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def preprocesTestTrain(combined_df):\n",
    "    # Step 1: Encode categorical features\n",
    "    label_encoder = LabelEncoder()\n",
    "    combined_df['eth.src'] = label_encoder.fit_transform(combined_df['eth.src'])\n",
    "    combined_df['IP.src'] = label_encoder.fit_transform(combined_df['IP.src'])\n",
    "    combined_df['IP.dst'] = label_encoder.fit_transform(combined_df['IP.dst'])\n",
    "    combined_df['Device Category'] = label_encoder.fit_transform(combined_df['Device Category'])\n",
    "\n",
    "\t\t\t\t\n",
    "    # Step 2: Prepare features and target variable; 'std_inter_arrival', 'std_packet_size', 'min_inter_arrival', 'min_packet_size', 'flow_duration' are removed\n",
    "    features = [ 'eth.src', 'IP.src', 'IP.dst', 'total_packets', 'total_bytes', 'mean_packet_size', \n",
    "                 'max_packet_size', 'packet_rate', 'byte_rate', 'mean_inter_arrival', \n",
    "                'max_inter_arrival']\n",
    "    X = combined_df[features]\n",
    "    return X\n",
    "\n",
    "def train_model(model, X_train, y_train):\n",
    "    start_time = time.time()\n",
    "    mem_usage = memory_usage((model.fit, (X_train, y_train)), interval=0.1)\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    peak_memory_usage = max(mem_usage)\n",
    "    return model, training_time, peak_memory_usage\n",
    "\n",
    "def predict_model(model, X_test, y_test):\n",
    "    start_time = time.time()\n",
    "    mem_usage = memory_usage((model.predict, (X_test,)), interval=0.1)\n",
    "    end_time = time.time()\n",
    "    prediction_time = end_time - start_time\n",
    "    peak_memory_usage = max(mem_usage)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = classification_report(y_test, y_pred)\n",
    "\n",
    "    return prediction_time, peak_memory_usage, accuracy\n",
    "\n",
    "def get_model_size(model):\n",
    "    joblib.dump(model, 'temp_model.joblib')\n",
    "    model_size = os.path.getsize('temp_model.joblib') / 1024**2  # size in MB\n",
    "    os.remove('temp_model.joblib')\n",
    "    return model_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "df1 = pd.read_csv('16-09-23.csv', low_memory=False)\n",
    "df2 = pd.read_csv('16-09-24.csv', low_memory=False)\n",
    "df3 = pd.read_csv('16-09-25.csv', low_memory=False)\n",
    "df4 = pd.read_csv('16-09-26.csv', low_memory=False)\n",
    "df5 = pd.read_csv('16-09-27.csv', low_memory=False)\n",
    "df6 = pd.read_csv('16-09-29.csv', low_memory=False)\n",
    "df7 = pd.read_csv('16-10-04.csv', low_memory=False)\n",
    "df8 = pd.read_csv('16-10-12.csv', low_memory=False)\n",
    "\n",
    "data = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8])\n",
    "data_60sec_train = process_network_traffic(data, '60S')\n",
    "# Save the final table as a CSV file\n",
    "#data_60sec_train.to_csv('Dataset_60sec.csv', index=False)\n",
    "\n",
    "X_60 = preprocesTestTrain(data_60sec_train)\n",
    "y_60 = data_60sec_train['Device Category']\n",
    "# Split the data into training and testing sets\n",
    "X_train_60, X_test_60, y_train_60, y_test_60 = train_test_split(X_60, y_60, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "df1 = pd.read_csv('16-09-23.csv', low_memory=False)\n",
    "df2 = pd.read_csv('16-09-24.csv', low_memory=False)\n",
    "df3 = pd.read_csv('16-09-25.csv', low_memory=False)\n",
    "df4 = pd.read_csv('16-09-26.csv', low_memory=False)\n",
    "df5 = pd.read_csv('16-09-27.csv', low_memory=False)\n",
    "df6 = pd.read_csv('16-09-29.csv', low_memory=False)\n",
    "df7 = pd.read_csv('16-10-04.csv', low_memory=False)\n",
    "df8 = pd.read_csv('16-10-12.csv', low_memory=False)\n",
    "\n",
    "data = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8])\n",
    "data_15sec_train = process_network_traffic(data, '15S')\n",
    "# Save the final table as a CSV file\n",
    "#data_60sec_train.to_csv('Dataset_60sec.csv', index=False)\n",
    "\n",
    "X_15 = preprocesTestTrain(data_15sec_train)\n",
    "y_15 = data_15sec_train['Device Category']\n",
    "# Split the data into training and testing sets\n",
    "X_train_15, X_test_15, y_train_15, y_test_15 = train_test_split(X_15, y_15, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for dataset 1: {'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Best parameters for dataset 2: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=20, random_state=100)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=20, random_state=100)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_depth=20, random_state=100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10, 12, 15, 20, 25, 30],\n",
    "    'min_samples_split': [2, 3, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4, 5, 6, 10]\n",
    "}\n",
    "\n",
    "# Initialize the classifier\n",
    "dt_clf = DecisionTreeClassifier(random_state=100)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=dt_clf, param_grid=param_grid, cv=5, n_jobs=-1, scoring='f1_macro')\n",
    "\n",
    "# Fit GridSearchCV on first dataset\n",
    "grid_search.fit(X_train_15, y_train_15)\n",
    "best_params_15 = grid_search.best_params_\n",
    "\n",
    "# Fit GridSearchCV on second dataset\n",
    "grid_search.fit(X_train_60, y_train_60)\n",
    "best_params_60 = grid_search.best_params_\n",
    "\n",
    "print(f\"Best parameters for dataset 1: {best_params_15}\")\n",
    "print(f\"Best parameters for dataset 2: {best_params_60}\")\n",
    "\n",
    "# Train the model with best parameters\n",
    "dt_clf_15 = DecisionTreeClassifier(**best_params_15, random_state=100)\n",
    "dt_clf_15.fit(X_train_15, y_train_15)\n",
    "\n",
    "dt_clf_60 = DecisionTreeClassifier(**best_params_60, random_state=100)\n",
    "dt_clf_60.fit(X_train_60, y_train_60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 18.06 seconds\n",
      "Training peak memory usage: 205.57 MB\n",
      "Training time: 9.70 seconds\n",
      "Training peak memory usage: 172.26 MB\n",
      "Prediction time: 1.5049 seconds\n",
      "Prediction peak memory usage: 173.18 MB\n",
      "Accuracy:               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7673\n",
      "           1       1.00      1.00      1.00     49240\n",
      "           2       1.00      1.00      1.00      2052\n",
      "           3       1.00      1.00      1.00      8101\n",
      "           4       1.00      1.00      1.00     24769\n",
      "           5       1.00      1.00      1.00      6080\n",
      "\n",
      "    accuracy                           1.00     97915\n",
      "   macro avg       1.00      1.00      1.00     97915\n",
      "weighted avg       1.00      1.00      1.00     97915\n",
      "\n",
      "Prediction time: 1.0490 seconds\n",
      "Prediction peak memory usage: 174.07 MB\n",
      "Accuracy:               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      6991\n",
      "           1       1.00      1.00      1.00     23572\n",
      "           2       1.00      1.00      1.00      2024\n",
      "           3       1.00      1.00      1.00      5191\n",
      "           4       1.00      1.00      1.00     10757\n",
      "           5       1.00      1.00      1.00      4150\n",
      "\n",
      "    accuracy                           1.00     52685\n",
      "   macro avg       1.00      1.00      1.00     52685\n",
      "weighted avg       1.00      1.00      1.00     52685\n",
      "\n",
      "Model size: 7.27 MB\n",
      "Model size: 7.27 MB\n"
     ]
    }
   ],
   "source": [
    "# Train the Random Forest Original:100, 10,20,5,100, second iteration: 50,15,20,t,100\n",
    "rf_clf =  RandomForestClassifier(n_estimators=100, max_depth=15,min_samples_split=5, min_samples_leaf=2,random_state=100)\n",
    "\n",
    "# Measure training time and memory usage\n",
    "rf_model_15, training_time, training_memory = train_model(rf_clf, X_train_15, y_train_15)\n",
    "print(f\"Training time: {training_time:.2f} seconds\")\n",
    "print(f\"Training peak memory usage: {training_memory:.2f} MB\")\n",
    "\n",
    "rf_model_60, training_time, training_memory = train_model(rf_clf, X_train_60, y_train_60)\n",
    "print(f\"Training time: {training_time:.2f} seconds\")\n",
    "print(f\"Training peak memory usage: {training_memory:.2f} MB\")\n",
    "\n",
    "\n",
    "# Measure prediction time, memory usage, and accuracy\n",
    "prediction_time, prediction_memory, accuracy = predict_model(rf_model_15, X_test_15, y_test_15)\n",
    "print(f\"Prediction time: {prediction_time:.4f} seconds\")\n",
    "print(f\"Prediction peak memory usage: {prediction_memory:.2f} MB\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "prediction_time, prediction_memory, accuracy = predict_model(rf_model_60, X_test_60, y_test_60)\n",
    "print(f\"Prediction time: {prediction_time:.4f} seconds\")\n",
    "print(f\"Prediction peak memory usage: {prediction_memory:.2f} MB\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Measure model size\n",
    "model_size = get_model_size(rf_model_15)\n",
    "print(f\"Model size: {model_size:.2f} MB\")\n",
    "\n",
    "model_size = get_model_size(rf_model_60)\n",
    "print(f\"Model size: {model_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Training time: 25.23 seconds\n",
      "Training peak memory usage: 194.02 MB\n",
      "Prediction time: 2.2357 seconds\n",
      "Prediction peak memory usage: 180.43 MB\n",
      "Accuracy:               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     10386\n",
      "           1       1.00      1.00      1.00     67114\n",
      "           2       1.00      1.00      1.00      2887\n",
      "           3       1.00      1.00      1.00     10431\n",
      "           4       1.00      1.00      1.00     34729\n",
      "           5       1.00      1.00      1.00      6555\n",
      "\n",
      "    accuracy                           1.00    132102\n",
      "   macro avg       1.00      1.00      1.00    132102\n",
      "weighted avg       1.00      1.00      1.00    132102\n",
      "\n",
      "Model size: 7.49 MB\n"
     ]
    }
   ],
   "source": [
    "# Train the Random Forest Original:100, 10,20,5,100, second iteration: 50,15,20,t,100\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [5, 10, 15, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=RandomForestClassifier(), param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "# Best model\n",
    "rf_clf = grid_search.best_estimator_\n",
    "\n",
    "# Measure training time and memory usage\n",
    "rf_model, training_time, training_memory = train_model(rf_clf, X_train, y_train)\n",
    "print(f\"Training time: {training_time:.2f} seconds\")\n",
    "print(f\"Training peak memory usage: {training_memory:.2f} MB\")\n",
    "\n",
    "# Measure prediction time, memory usage, and accuracy\n",
    "prediction_time, prediction_memory, accuracy = predict_model(rf_model, X_test, y_test)\n",
    "print(f\"Prediction time: {prediction_time:.4f} seconds\")\n",
    "print(f\"Prediction peak memory usage: {prediction_memory:.2f} MB\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Measure model size\n",
    "model_size = get_model_size(rf_model)\n",
    "print(f\"Model size: {model_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mitak\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m9792/9792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1ms/step - accuracy: 0.7827 - loss: 0.6251 - val_accuracy: 0.9850 - val_loss: 0.0710\n",
      "Epoch 2/50\n",
      "\u001b[1m9792/9792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - accuracy: 0.9481 - loss: 0.1695 - val_accuracy: 0.9921 - val_loss: 0.0381\n",
      "Epoch 3/50\n",
      "\u001b[1m9792/9792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - accuracy: 0.9627 - loss: 0.1251 - val_accuracy: 0.9934 - val_loss: 0.0305\n",
      "Epoch 4/50\n",
      "\u001b[1m9792/9792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1ms/step - accuracy: 0.9697 - loss: 0.1061 - val_accuracy: 0.9956 - val_loss: 0.0228\n",
      "Epoch 5/50\n",
      "\u001b[1m9792/9792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - accuracy: 0.9730 - loss: 0.1015 - val_accuracy: 0.9969 - val_loss: 0.0224\n",
      "Epoch 6/50\n",
      "\u001b[1m9792/9792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - accuracy: 0.9759 - loss: 0.0879 - val_accuracy: 0.9976 - val_loss: 0.0161\n",
      "Epoch 7/50\n",
      "\u001b[1m9792/9792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1ms/step - accuracy: 0.9778 - loss: 0.0893 - val_accuracy: 0.9981 - val_loss: 0.0135\n",
      "Epoch 8/50\n",
      "\u001b[1m9792/9792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - accuracy: 0.9793 - loss: 0.0822 - val_accuracy: 0.9981 - val_loss: 0.0173\n",
      "Epoch 9/50\n",
      "\u001b[1m9792/9792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - accuracy: 0.9800 - loss: 0.0748 - val_accuracy: 0.9984 - val_loss: 0.0126\n",
      "Epoch 10/50\n",
      "\u001b[1m9792/9792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - accuracy: 0.9808 - loss: 0.0740 - val_accuracy: 0.9982 - val_loss: 0.0146\n",
      "Epoch 11/50\n",
      "\u001b[1m9792/9792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - accuracy: 0.9818 - loss: 0.0715 - val_accuracy: 0.9984 - val_loss: 0.0090\n",
      "Epoch 12/50\n",
      "\u001b[1m9792/9792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1ms/step - accuracy: 0.9828 - loss: 0.0689 - val_accuracy: 0.9982 - val_loss: 0.0171\n",
      "Epoch 13/50\n",
      "\u001b[1m9792/9792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - accuracy: 0.9831 - loss: 0.0721 - val_accuracy: 0.9985 - val_loss: 0.0100\n",
      "Epoch 14/50\n",
      "\u001b[1m9792/9792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1ms/step - accuracy: 0.9838 - loss: 0.0635 - val_accuracy: 0.9984 - val_loss: 0.0127\n",
      "Epoch 15/50\n",
      "\u001b[1m9792/9792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - accuracy: 0.9845 - loss: 0.0640 - val_accuracy: 0.9986 - val_loss: 0.0097\n",
      "Epoch 16/50\n",
      "\u001b[1m9792/9792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1ms/step - accuracy: 0.9849 - loss: 0.0640 - val_accuracy: 0.9984 - val_loss: 0.0088\n",
      "Epoch 17/50\n",
      "\u001b[1m9792/9792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - accuracy: 0.9856 - loss: 0.0579 - val_accuracy: 0.9987 - val_loss: 0.0109\n",
      "Epoch 18/50\n",
      "\u001b[1m9792/9792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1ms/step - accuracy: 0.9856 - loss: 0.0618 - val_accuracy: 0.9986 - val_loss: 0.0089\n",
      "Epoch 19/50\n",
      "\u001b[1m9792/9792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - accuracy: 0.9858 - loss: 0.0598 - val_accuracy: 0.9985 - val_loss: 0.0105\n",
      "Epoch 20/50\n",
      "\u001b[1m9792/9792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - accuracy: 0.9865 - loss: 0.0548 - val_accuracy: 0.9984 - val_loss: 0.0114\n",
      "Epoch 21/50\n",
      "\u001b[1m9792/9792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - accuracy: 0.9857 - loss: 0.0589 - val_accuracy: 0.9986 - val_loss: 0.0080\n",
      "Epoch 22/50\n",
      "\u001b[1m9792/9792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - accuracy: 0.9856 - loss: 0.0599 - val_accuracy: 0.9985 - val_loss: 0.0097\n",
      "Epoch 23/50\n",
      "\u001b[1m9792/9792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - accuracy: 0.9865 - loss: 0.0544 - val_accuracy: 0.9985 - val_loss: 0.0083\n",
      "Epoch 24/50\n",
      "\u001b[1m9792/9792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - accuracy: 0.9868 - loss: 0.0546 - val_accuracy: 0.9986 - val_loss: 0.0111\n",
      "Epoch 25/50\n",
      "\u001b[1m9792/9792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - accuracy: 0.9868 - loss: 0.0530 - val_accuracy: 0.9987 - val_loss: 0.0102\n",
      "Epoch 26/50\n",
      "\u001b[1m9792/9792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - accuracy: 0.9868 - loss: 0.0555 - val_accuracy: 0.9988 - val_loss: 0.0091\n",
      "Epoch 27/50\n",
      "\u001b[1m9792/9792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - accuracy: 0.9864 - loss: 0.0609 - val_accuracy: 0.9987 - val_loss: 0.0100\n",
      "Epoch 28/50\n",
      "\u001b[1m9792/9792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - accuracy: 0.9865 - loss: 0.0599 - val_accuracy: 0.9985 - val_loss: 0.0084\n",
      "Epoch 29/50\n",
      "\u001b[1m9792/9792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1ms/step - accuracy: 0.9868 - loss: 0.0544 - val_accuracy: 0.9988 - val_loss: 0.0091\n",
      "Epoch 30/50\n",
      "\u001b[1m9792/9792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1ms/step - accuracy: 0.9878 - loss: 0.0517 - val_accuracy: 0.9989 - val_loss: 0.0132\n",
      "Epoch 31/50\n",
      "\u001b[1m9792/9792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1ms/step - accuracy: 0.9867 - loss: 0.0550 - val_accuracy: 0.9988 - val_loss: 0.0121\n",
      "\u001b[1m3060/3060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 816us/step - accuracy: 0.9984 - loss: 0.0088\n",
      "Test accuracy: 0.9985\n",
      "\u001b[1m3060/3060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 822us/step\n",
      "True label: 4, Predicted label: 4\n",
      "True label: 4, Predicted label: 4\n",
      "True label: 0, Predicted label: 0\n",
      "True label: 4, Predicted label: 4\n",
      "True label: 4, Predicted label: 4\n",
      "True label: 1, Predicted label: 1\n",
      "True label: 4, Predicted label: 4\n",
      "True label: 1, Predicted label: 1\n",
      "True label: 0, Predicted label: 0\n",
      "True label: 1, Predicted label: 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "X = preprocesTestTrain(data_15sec_train)\n",
    "y = data_15sec_train['Device Category']\n",
    "\n",
    "# Encode the target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_categorical = to_categorical(y_encoded)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=100)\n",
    "\n",
    "# Standardize the feature columns\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Build the neural network model\n",
    "model = Sequential([\n",
    "    Dense(128, input_shape=(X_train_scaled.shape[1],), activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(y_categorical.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Set up early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=50, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Decode the predicted and true classes\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred_classes)\n",
    "y_true_labels = label_encoder.inverse_transform(y_true_classes)\n",
    "\n",
    "# Display the first few predictions\n",
    "for i in range(10):\n",
    "    print(f\"True label: {y_true_labels[i]}, Predicted label: {y_pred_labels[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mitak\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.6994 - loss: 0.8325 - val_accuracy: 0.9544 - val_loss: 0.1466\n",
      "Epoch 2/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.9159 - loss: 0.2693 - val_accuracy: 0.9837 - val_loss: 0.0672\n",
      "Epoch 3/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.9432 - loss: 0.1899 - val_accuracy: 0.9910 - val_loss: 0.0465\n",
      "Epoch 4/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.9554 - loss: 0.1540 - val_accuracy: 0.9939 - val_loss: 0.0320\n",
      "Epoch 5/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.9646 - loss: 0.1315 - val_accuracy: 0.9958 - val_loss: 0.0275\n",
      "Epoch 6/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.9671 - loss: 0.1229 - val_accuracy: 0.9953 - val_loss: 0.0242\n",
      "Epoch 7/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.9706 - loss: 0.1055 - val_accuracy: 0.9960 - val_loss: 0.0237\n",
      "Epoch 8/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.9721 - loss: 0.1077 - val_accuracy: 0.9964 - val_loss: 0.0202\n",
      "Epoch 9/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.9747 - loss: 0.1012 - val_accuracy: 0.9963 - val_loss: 0.0194\n",
      "Epoch 10/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.9757 - loss: 0.0961 - val_accuracy: 0.9968 - val_loss: 0.0177\n",
      "Epoch 11/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.9762 - loss: 0.0926 - val_accuracy: 0.9968 - val_loss: 0.0173\n",
      "Epoch 12/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.9779 - loss: 0.0869 - val_accuracy: 0.9968 - val_loss: 0.0177\n",
      "Epoch 13/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.9779 - loss: 0.0886 - val_accuracy: 0.9972 - val_loss: 0.0162\n",
      "Epoch 14/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.9783 - loss: 0.0866 - val_accuracy: 0.9968 - val_loss: 0.0151\n",
      "Epoch 15/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.9801 - loss: 0.0763 - val_accuracy: 0.9967 - val_loss: 0.0147\n",
      "Epoch 16/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.9799 - loss: 0.0781 - val_accuracy: 0.9964 - val_loss: 0.0181\n",
      "Epoch 17/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.9801 - loss: 0.0743 - val_accuracy: 0.9969 - val_loss: 0.0152\n",
      "Epoch 18/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.9803 - loss: 0.0760 - val_accuracy: 0.9970 - val_loss: 0.0129\n",
      "Epoch 19/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.9814 - loss: 0.0755 - val_accuracy: 0.9970 - val_loss: 0.0136\n",
      "Epoch 20/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.9814 - loss: 0.0760 - val_accuracy: 0.9976 - val_loss: 0.0139\n",
      "Epoch 21/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.9827 - loss: 0.0699 - val_accuracy: 0.9970 - val_loss: 0.0130\n",
      "Epoch 22/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.9818 - loss: 0.0746 - val_accuracy: 0.9970 - val_loss: 0.0137\n",
      "Epoch 23/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.9829 - loss: 0.0793 - val_accuracy: 0.9969 - val_loss: 0.0121\n",
      "Epoch 24/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.9831 - loss: 0.0713 - val_accuracy: 0.9973 - val_loss: 0.0126\n",
      "Epoch 25/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.9823 - loss: 0.0828 - val_accuracy: 0.9971 - val_loss: 0.0129\n",
      "Epoch 26/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.9822 - loss: 0.0719 - val_accuracy: 0.9971 - val_loss: 0.0106\n",
      "Epoch 27/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.9825 - loss: 0.0698 - val_accuracy: 0.9970 - val_loss: 0.0111\n",
      "Epoch 28/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.9836 - loss: 0.0669 - val_accuracy: 0.9975 - val_loss: 0.0106\n",
      "Epoch 29/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.9844 - loss: 0.0741 - val_accuracy: 0.9977 - val_loss: 0.0103\n",
      "Epoch 30/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.9843 - loss: 0.0645 - val_accuracy: 0.9984 - val_loss: 0.0105\n",
      "Epoch 31/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.9838 - loss: 0.0721 - val_accuracy: 0.9976 - val_loss: 0.0101\n",
      "Epoch 32/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.9841 - loss: 0.0712 - val_accuracy: 0.9974 - val_loss: 0.0110\n",
      "Epoch 33/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.9847 - loss: 0.0650 - val_accuracy: 0.9981 - val_loss: 0.0091\n",
      "Epoch 34/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.9842 - loss: 0.0631 - val_accuracy: 0.9977 - val_loss: 0.0094\n",
      "Epoch 35/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.9853 - loss: 0.0630 - val_accuracy: 0.9981 - val_loss: 0.0095\n",
      "Epoch 36/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.9842 - loss: 0.0733 - val_accuracy: 0.9979 - val_loss: 0.0102\n",
      "Epoch 37/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.9853 - loss: 0.0672 - val_accuracy: 0.9983 - val_loss: 0.0086\n",
      "Epoch 38/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.9852 - loss: 0.0616 - val_accuracy: 0.9986 - val_loss: 0.0084\n",
      "Epoch 39/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.9856 - loss: 0.0622 - val_accuracy: 0.9983 - val_loss: 0.0083\n",
      "Epoch 40/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.9857 - loss: 0.0614 - val_accuracy: 0.9986 - val_loss: 0.0087\n",
      "Epoch 41/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.9866 - loss: 0.0544 - val_accuracy: 0.9981 - val_loss: 0.0079\n",
      "Epoch 42/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.9864 - loss: 0.0543 - val_accuracy: 0.9982 - val_loss: 0.0094\n",
      "Epoch 43/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.9858 - loss: 0.0589 - val_accuracy: 0.9986 - val_loss: 0.0079\n",
      "Epoch 44/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.9871 - loss: 0.0597 - val_accuracy: 0.9984 - val_loss: 0.0095\n",
      "Epoch 45/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.9863 - loss: 0.0612 - val_accuracy: 0.9986 - val_loss: 0.0087\n",
      "Epoch 46/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.9859 - loss: 0.0614 - val_accuracy: 0.9986 - val_loss: 0.0090\n",
      "Epoch 47/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.9878 - loss: 0.0551 - val_accuracy: 0.9986 - val_loss: 0.0069\n",
      "Epoch 48/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.9863 - loss: 0.0603 - val_accuracy: 0.9983 - val_loss: 0.0073\n",
      "Epoch 49/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.9870 - loss: 0.0551 - val_accuracy: 0.9984 - val_loss: 0.0073\n",
      "Epoch 50/50\n",
      "\u001b[1m5269/5269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.9870 - loss: 0.0578 - val_accuracy: 0.9988 - val_loss: 0.0057\n",
      "\u001b[1m1647/1647\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 663us/step - accuracy: 0.9988 - loss: 0.0050\n",
      "Test accuracy: 0.9989\n",
      "\u001b[1m1647/1647\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 810us/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "X = preprocesTestTrain(data_60sec_train)\n",
    "y = data_60sec_train['Device Category']\n",
    "\n",
    "# Encode the target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_categorical = to_categorical(y_encoded)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=100)\n",
    "\n",
    "# Standardize the feature columns\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Build the neural network model\n",
    "model = Sequential([\n",
    "    Dense(128, input_shape=(X_train_scaled.shape[1],), activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(y_categorical.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Set up early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=50, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Decode the predicted and true classes\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred_classes)\n",
    "y_true_labels = label_encoder.inverse_transform(y_true_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model on new data\n",
    "new_df1 = pd.read_csv('16-09-30.csv', low_memory=False)\n",
    "new_df2 = pd.read_csv('16-10-01.csv', low_memory=False)\n",
    "new_df3 = pd.read_csv('16-10-02.csv', low_memory=False)\n",
    "new_df4 = pd.read_csv('16-10-03.csv', low_memory=False)\n",
    "new_df5 = pd.read_csv('16-10-05.csv', low_memory=False)\n",
    "\n",
    "# Skip the first row (header row with explanations)\n",
    "new_df1 = new_df1.iloc[1:]\n",
    "new_df2 = new_df2.iloc[1:]\n",
    "new_df3 = new_df3.iloc[1:]\n",
    "new_df4 = new_df4.iloc[1:]\n",
    "new_df5 = new_df5.iloc[1:]\n",
    "\n",
    "\n",
    "# Concatenate the dataframes\n",
    "new_df = pd.concat([new_df1,new_df2, new_df3, new_df4, new_df5], ignore_index=True)\n",
    "# 60sec interval\n",
    "new_df_60 = process_network_traffic(new_df, '60S')\n",
    "X_new_60 = preprocesTestTrain(new_df_60)\n",
    "\n",
    "# Assuming you have ground truth labels in the new dataset\n",
    "y_new_true_60 = new_df_60['Device Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model on new data\n",
    "new_df1 = pd.read_csv('16-09-30.csv', low_memory=False)\n",
    "new_df2 = pd.read_csv('16-10-01.csv', low_memory=False)\n",
    "new_df3 = pd.read_csv('16-10-02.csv', low_memory=False)\n",
    "new_df4 = pd.read_csv('16-10-03.csv', low_memory=False)\n",
    "new_df5 = pd.read_csv('16-10-05.csv', low_memory=False)\n",
    "\n",
    "# Skip the first row (header row with explanations)\n",
    "new_df1 = new_df1.iloc[1:]\n",
    "new_df2 = new_df2.iloc[1:]\n",
    "new_df3 = new_df3.iloc[1:]\n",
    "new_df4 = new_df4.iloc[1:]\n",
    "new_df5 = new_df5.iloc[1:]\n",
    "\n",
    "\n",
    "# Concatenate the dataframes\n",
    "new_df = pd.concat([new_df1,new_df2, new_df3, new_df4, new_df5], ignore_index=True)\n",
    "\n",
    "# 15sec interval\n",
    "new_df_15 = process_network_traffic(new_df, '15S')\n",
    "X_new_15 = preprocesTestTrain(new_df_15)\n",
    "\n",
    "# Assuming you have ground truth labels in the new dataset\n",
    "y_new_true_15 = new_df_15['Device Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "60 sec interval\n",
      "\n",
      "Prediction time: 2.9732 seconds\n",
      "Prediction peak memory usage: 169.34 MB\n",
      "Accuracy:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.83      0.67     35134\n",
      "           1       1.00      0.89      0.94    123434\n",
      "           2       0.02      0.04      0.03      7130\n",
      "           3       0.94      0.98      0.96     23063\n",
      "           4       0.00      0.00      0.00     35062\n",
      "           5       0.28      0.42      0.33     21218\n",
      "\n",
      "    accuracy                           0.70    245041\n",
      "   macro avg       0.47      0.53      0.49    245041\n",
      "weighted avg       0.70      0.70      0.69    245041\n",
      "\n",
      "\n",
      "15 sec interval\n",
      "\n",
      "Prediction time: 1.2109 seconds\n",
      "Prediction peak memory usage: 215.88 MB\n",
      "Accuracy:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.78      0.48     38435\n",
      "           1       1.00      0.85      0.92    233973\n",
      "           2       0.03      0.02      0.02      7505\n",
      "           3       0.73      0.99      0.84     46768\n",
      "           4       0.75      0.58      0.65     80470\n",
      "           5       0.79      0.63      0.70     31966\n",
      "\n",
      "    accuracy                           0.78    439117\n",
      "   macro avg       0.61      0.64      0.60    439117\n",
      "weighted avg       0.84      0.78      0.79    439117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict and measure prediction time, memory usage, and accuracy: Decision Tree\n",
    "# 60sec interval\n",
    "print(\"\\n60 sec interval\\n\")\n",
    "prediction_time, prediction_memory, accuracy = predict_model(dt_clf_60, X_new_60, y_new_true_60)\n",
    "print(f\"Prediction time: {prediction_time:.4f} seconds\")\n",
    "print(f\"Prediction peak memory usage: {prediction_memory:.2f} MB\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# 15sec interval\n",
    "print(\"\\n15 sec interval\\n\")\n",
    "prediction_time, prediction_memory, accuracy = predict_model(dt_clf_15, X_new_15, y_new_true_15)\n",
    "print(f\"Prediction time: {prediction_time:.4f} seconds\")\n",
    "print(f\"Prediction peak memory usage: {prediction_memory:.2f} MB\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "60 sec interval\n",
      "\n",
      "Prediction time: 3.0330 seconds\n",
      "Prediction peak memory usage: 144.91 MB\n",
      "Accuracy:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.93     35134\n",
      "           1       1.00      0.96      0.98    123434\n",
      "           2       0.98      0.84      0.90      7130\n",
      "           3       0.94      0.97      0.96     23063\n",
      "           4       0.78      0.93      0.85     35062\n",
      "           5       0.84      0.70      0.77     21218\n",
      "\n",
      "    accuracy                           0.93    245041\n",
      "   macro avg       0.91      0.89      0.90    245041\n",
      "weighted avg       0.93      0.93      0.93    245041\n",
      "\n",
      "\n",
      "15 sec interval\n",
      "\n",
      "Prediction time: 3.6233 seconds\n",
      "Prediction peak memory usage: 195.73 MB\n",
      "Accuracy:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.95      0.72     38435\n",
      "           1       0.97      0.89      0.93    233973\n",
      "           2       0.96      0.79      0.87      7505\n",
      "           3       0.96      0.97      0.96     46768\n",
      "           4       0.78      0.45      0.57     80470\n",
      "           5       0.35      0.69      0.47     31966\n",
      "\n",
      "    accuracy                           0.81    439117\n",
      "   macro avg       0.77      0.79      0.75    439117\n",
      "weighted avg       0.86      0.81      0.82    439117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict and measure prediction time, memory usage, and accuracy: Random Forest\n",
    "# 60sec interval\n",
    "print(\"\\n60 sec interval\\n\")\n",
    "prediction_time, prediction_memory, accuracy = predict_model(rf_model_60, X_new_60, y_new_true_60)\n",
    "print(f\"Prediction time: {prediction_time:.4f} seconds\")\n",
    "print(f\"Prediction peak memory usage: {prediction_memory:.2f} MB\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# 15sec interval\n",
    "print(\"\\n15 sec interval\\n\")\n",
    "prediction_time, prediction_memory, accuracy = predict_model(rf_model_15, X_new_15, y_new_true_15)\n",
    "print(f\"Prediction time: {prediction_time:.4f} seconds\")\n",
    "print(f\"Prediction peak memory usage: {prediction_memory:.2f} MB\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13723/13723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 683us/step\n",
      "Test accuracy on new data: 0.9102\n",
      "\u001b[1m7658/7658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 695us/step\n",
      "Test accuracy on new data: 0.9021\n"
     ]
    }
   ],
   "source": [
    "# 15sec interval\n",
    "# Encode the target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_true_encoded = label_encoder.fit_transform(y_new_true_15)\n",
    "y_true_categorical = to_categorical(y_true_encoded)\n",
    "\n",
    "# Standardize the feature columns using the same scaler used for training\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X)  # X from the training phase\n",
    "X_new_scaled = scaler.transform(X_new_15)\n",
    "\n",
    "# Make predictions on the new data\n",
    "y_pred = model.predict(X_new_scaled)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_true_categorical, axis=1)\n",
    "\n",
    "# Decode the predicted and true classes\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred_classes)\n",
    "y_true_labels = label_encoder.inverse_transform(y_true_classes)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
    "print(f\"Test accuracy on new data: {accuracy:.4f}\")\n",
    "\n",
    "# 60sec interval\n",
    "# Encode the target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_true_encoded = label_encoder.fit_transform(y_new_true_60)\n",
    "y_true_categorical = to_categorical(y_true_encoded)\n",
    "\n",
    "# Standardize the feature columns using the same scaler used for training\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X)  # X from the training phase\n",
    "X_new_scaled = scaler.transform(X_new_60)\n",
    "\n",
    "# Make predictions on the new data\n",
    "y_pred = model.predict(X_new_scaled)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_true_categorical, axis=1)\n",
    "\n",
    "# Decode the predicted and true classes\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred_classes)\n",
    "y_true_labels = label_encoder.inverse_transform(y_true_classes)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
    "print(f\"Test accuracy on new data: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
